{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取的資料、斷詞\n",
    "\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "titles = pd.read_csv(\"ptt_artical.csv\")\n",
    "no_dup_titles = titles.drop_duplicates(subset =\"title\") \n",
    "num_class = 7\n",
    "replace_dir = {\"category\": {\"Gossiping\":0 ,\"Beauty\":1 , \"joke\":2, \"C_Chat\":3 , \"AC_In\":4 , \"marvel\":5 , \"StupidClown\":6}}\n",
    "\n",
    "no_dup_titles = no_dup_titles.replace(replace_dir)\n",
    "no_dup_t = no_dup_titles[\"title\"].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "titles = []\n",
    "\n",
    "count = 0\n",
    "max_len = -1\n",
    "\n",
    "\n",
    "for i in range(len(no_dup_t)):\n",
    "    len_counter = 0\n",
    "    seg_list = jieba.cut(no_dup_t[i])\n",
    "    \n",
    "    t = []\n",
    "    \n",
    "    \n",
    "    for s in seg_list:    \n",
    "        \n",
    "        len_counter+=1\n",
    "            \n",
    "        t.append(s)\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    titles.append(t)    \n",
    "    max_len = max(len_counter , max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2Vec 訓練\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "word_dim = 175\n",
    "model = Word2Vec(sentences=titles, size=word_dim, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(titles, total_examples=1, epochs=125)\n",
    "\n",
    "vector = model.wv['林瑋豐'] # 測試word2vec表現\n",
    "sims = model.wv.most_similar('林瑋豐', topn=15)  # get other similar words \n",
    "print(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_x = no_dup_titles[\"title\"]\n",
    "train_y = no_dup_titles[\"category\"]\n",
    "train_x=train_x.to_numpy()\n",
    "train_y=train_y.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "temp = []\n",
    "\n",
    "\n",
    "count = 0\n",
    "max_len = -1\n",
    "\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    len_counter = 0\n",
    "    seg_list = jieba.cut(train_x[i])\n",
    "    \n",
    "    t = []\n",
    "    \n",
    "    \n",
    "    for s in seg_list:    \n",
    "        \n",
    "        len_counter+=1\n",
    "        \n",
    "            \n",
    "        t.append(model.wv[s])\n",
    "        \n",
    "    \n",
    "    t=np.array(t , dtype=np.float)\n",
    "    \n",
    "    temp.append(t)    \n",
    "    max_len = max(len_counter , max_len)\n",
    "\n",
    "\n",
    "train_x = np.array(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "n_sample = len(train_x)\n",
    "\n",
    "\n",
    "temp = []\n",
    "ans = []\n",
    "seq_len = 5\n",
    "\n",
    "for i in range(len(train_x)):\n",
    "    for j in range(len(train_x[i])-seq_len):\n",
    "        temp.append(train_x[i][j:j+seq_len])\n",
    "        ans.append(train_y[i])\n",
    "        \n",
    "    \n",
    "\n",
    "temp = np.array(temp,dtype=\"float32\")\n",
    "ans = np.array(ans,dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(128, input_shape=(seq_len,word_dim)),\n",
    "    tf.keras.layers.Dense(64, activation='relu',),\n",
    "    tf.keras.layers.Dense(7, activation=\"softmax\", name=\"out\"),\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "predictor.compile(optimizer=opt,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "predictor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.shuffle(temp)\n",
    "\n",
    "indices = tf.range(start=0, limit=tf.shape(temp)[0], dtype=tf.int32)\n",
    "\n",
    "\n",
    "idx = tf.random.shuffle(indices)\n",
    "x_data = tf.gather(temp, idx)\n",
    "y_data = tf.gather(ans, idx)\n",
    "\n",
    "\n",
    "predictor.fit(x_data, y_data, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_dir = {\"category\": {\"Gossiping\":0 ,\"Beauty\":1 , \"joke\":2, \"C_Chat\":3 , \"AC_In\":4 , \"marvel\":5 , \"StupidClown\":6}}\n",
    "class_dir = {0:\"Gossiping\" , 1:\"Beauty\" , 2:\"joke\" , 3:\"C_Chat\" , 4:\"AC_In\" , 5:\"marvel\" , 6:\"StupidClown\" }\n",
    "\n",
    "\n",
    "def predict(word,model,word2Vec):\n",
    "\n",
    "\n",
    "        \n",
    "    t= []\n",
    "    seg_list = jieba.cut(word)\n",
    "    for s in seg_list:    \n",
    "        t.append(word2Vec.wv[s])\n",
    "        \n",
    "    t=np.array(t , dtype=np.float)\n",
    "    temp = []\n",
    "    \n",
    "    \n",
    "    for i in range(len(t)-seq_len):  \n",
    "        temp.append(t[i:i+seq_len])          \n",
    "        \n",
    "        \n",
    "    temp=np.array(temp , dtype=np.float)\n",
    "    output = model.predict(temp)\n",
    "\n",
    "\n",
    "    return output\n",
    "\n",
    "word = \"[問卦]不戴口罩的人是不是很糟糕\"\n",
    "\n",
    "out = predict(word, predictor , model)\n",
    "for i in out:\n",
    "  print(class_dir[np.argmax(i)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
