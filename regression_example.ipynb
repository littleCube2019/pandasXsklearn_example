{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import moduel and data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# 自用模組\n",
    "\n",
    "\n",
    "def get_num_data(df):\n",
    "    return df.select_dtypes(\"number\")\n",
    "\n",
    "def get_nonNum_data(df):\n",
    "    return df.select_dtypes(exclude=\"number\")\n",
    "\n",
    "#                                  ==== category data 相關    ====  \n",
    "\n",
    "#放入\"全部\"資料，與預測標的名稱  \n",
    "# 回傳 None ,但於stdout上顯示各項指標\n",
    "def catData_show_statistic(df,y_name):\n",
    "    \n",
    "    for i in df.select_dtypes(exclude=\"number\"):\n",
    "        miss = df.isna().mean(axis=0).to_frame(\"miss_rate\").sort_values(\"miss_rate\",ascending=False).transpose()\n",
    "        print(\"==========================\")\n",
    "        print(\"欄位:\",i)\n",
    "        \n",
    "        print(\"資料缺失率:\",miss[i][\"miss_rate\"])\n",
    "        \n",
    "        \n",
    "        counts = df[i].value_counts()\n",
    "        print(\"共\",len(counts),\"種資料\\n\") \n",
    "        for c in counts.index:\n",
    "            print(c, \"數量:\",counts[c])\n",
    "            print(\"此類型的y平均數\", df[y_name][df[i] == c].to_frame(\"1\").mean()[\"1\"] )\n",
    "            print(\"此類型的y中位數\", df[y_name][df[i] == c].to_frame(\"1\").median()[\"1\"] )\n",
    "            \n",
    "#放入\"全部\"資料，與預測標的名稱  \n",
    "# 回傳 將類別資料改成其類別對y的中位數 (資料缺失填入0)  與 被更改的類別label-> value 的dir\n",
    "#                                                      供接下來testing使用replace(dir)\n",
    "def catData_encoding(df,y_name):\n",
    "    replace_dir = {}\n",
    "    for i in df.select_dtypes(exclude=\"number\"):\n",
    "        counts = df[i].value_counts()\n",
    "        \n",
    "        \n",
    "        \n",
    "        replace_dir[i] = {}\n",
    "        \n",
    "        for c in counts.index:\n",
    "            replace_dir[i][c] = df[y_name][df[i] == c].to_frame(\"1\").mean()[\"1\"]\n",
    "\n",
    "    return df.replace(replace_dir),replace_dir\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#                                  ====   missing rate 相關   ====                    #\n",
    "\n",
    "def show_miss_rate(df):\n",
    "    miss = df.isna().mean(axis=0).to_frame(\"miss_rate\")\n",
    "    miss = miss[miss.miss_rate!=0].sort_values(\"miss_rate\",ascending=False)\n",
    "    sns.catplot(kind=\"bar\",data=miss.transpose(),aspect=2,height=10)\n",
    "    print(miss)\n",
    "# 印出所有含有na或null的標籤(特徵)，並依缺失率高低排序 \n",
    "# 同時畫出表格\n",
    "\n",
    "def drop_miss_rate(df,threshold):\n",
    "    miss = df.isna().mean(axis=0).to_frame(\"miss_rate\")\n",
    "    miss = miss[miss.miss_rate!=0].sort_values(\"miss_rate\",ascending=False).transpose()\n",
    "    \n",
    "    miss_index = []\n",
    "    for i in miss:\n",
    "        if (miss[i]>threshold).bool():\n",
    "            miss_index.append(i)\n",
    "    return df.drop(miss_index , axis=1)\n",
    "# 回傳丟掉高於threshold遺失率的欄位  \n",
    "# 0 <= threshold <= 1\n",
    "\n",
    "\n",
    "\n",
    "#   ===================================   for num data   ===================================  # \n",
    "#                                  ====   分布相關   ====                    #\n",
    "\n",
    "    \n",
    "\n",
    "def numData_show_statistic(df,y_name):\n",
    "    print(\"比較標的:常態分佈 偏度與峰度皆為0\")\n",
    "    print(\"偏度很大=>極度右偏 ， 峰度很大 => 值集中\")\n",
    "    miss = df.isna().mean(axis=0).to_frame(\"miss_rate\").sort_values(\"miss_rate\",ascending=False).transpose()\n",
    "\n",
    "    for i in df:\n",
    "        print(\"==========================\")\n",
    "        print(\"欄位:\",i)\n",
    "        print(\"    ---     \")\n",
    "        print(\"平均值\",df[i].mean())\n",
    "        print(\"標準差\",df[i].std())\n",
    "        print(\"偏度:\",df[i].skew())\n",
    "        print(\"峰度:\",df[i].skew())\n",
    "        print(\"    ---     \")\n",
    "        print(\"資料缺失率:\",miss[i][\"miss_rate\"])\n",
    "        print(\"    ---     \")\n",
    "        print(\"Pearson相關係數(共線程度):\",df.corr(method='pearson')[y_name][i])\n",
    "        print(\"Spearman相關係數(廣義關係):\",df.corr(method='spearman')[y_name][i])\n",
    "        print(\"Kendall相關係數(單調關係):\",df.corr(method='kendall')[y_name][i])\n",
    "# 照欄位順序依次印出基本統計數據\n",
    "\n",
    "#如名，印出y_mean 與 y_std 關係，確認是否要取log  \n",
    "#(如果無關=>不用調  正比就開根、正二次方就取log)\n",
    "def show_y_mean_and_std_relation(df,y_name):\n",
    "    temp = {}\n",
    "    temp[\"std\"] = []\n",
    "    temp[\"mu\"] = []\n",
    "    \n",
    "    print(\"Y偏度:\",df[y_name].skew())\n",
    "    print(\"Y峰度:\",df[y_name].kurt())\n",
    "\n",
    "    #pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "    for i in range(len(df[y_name])):\n",
    "\n",
    "        temp[\"mu\"].append(df[y_name][:i+1].mean())\n",
    "        temp[\"std\"].append(df[y_name][:i+1].std()*train_y[:i+1].std()  )\n",
    "\n",
    "    temp = pd.DataFrame(temp) \n",
    "    print(temp)\n",
    "\n",
    "    #from matplotlib import pyplot as plt\n",
    "    #f, ax = plt.subplots()\n",
    "    #ax.ticklabel_format(style='plain', axis='both')\n",
    "    sns.relplot(x=\"std\", y=\"mu\", data=temp)\n",
    "\n",
    "\n",
    "def numData_show_detail(df,x_name,y_name):\n",
    "    des = df.describe()\n",
    "    \n",
    "    print(\"===============與目標關係程度===============\\n\\n\")\n",
    "    print(\"Pearson相關係數(共線程度):\",df.corr(method='pearson')[y_name][x_name])\n",
    "    print(\"Spearman相關係數(單調關係):\",df.corr(method='spearman')[y_name][x_name])\n",
    "    print(\"Kendall相關係數(單調關係):\",df.corr(method='kendall')[y_name][x_name])\n",
    "    print(\"\\n\")\n",
    "    print(\"================基本數值================\\n\\n\")\n",
    "    print(des[x_name])\n",
    "    print(\"\\n\")\n",
    "    print(\"X偏度:\",df[x_name].skew())\n",
    "    print(\"X峰度:\",df[x_name].skew())\n",
    "    print(\"================與目標關係圖================\\n\\n\")\n",
    "    sns.relplot(x=x_name, y=y_name, data=df)\n",
    "    print(\"\\n\")\n",
    "# 單欄位詳細資料\n",
    "\n",
    "# 輸入想要的correlation係數絕對值門檻，指定方法\n",
    "# 回傳高於門檻的 \"index label\"(list)  \n",
    "def numData_corr_filter(df,y_name,threshold,kind=\"spearman\"):\n",
    "    train_x_label = []\n",
    "    for i in df:\n",
    " \n",
    "        if abs(df.corr(method=kind)[y_name][i]) > threshold :\n",
    "             if i != y_name:\n",
    "                train_x_label.append(i)\n",
    "    return train_x_label\n",
    "\n",
    "\n",
    "# 限制所有值在n個標準差之內\n",
    "# 回傳限制完的結果(df)\n",
    "def numData_outlier_limit(df ,y_name , n):\n",
    "    \n",
    "    #label_counter = 0\n",
    "    #total_labels = len(df.columns)\n",
    "    \n",
    "    for i in df:\n",
    "        \n",
    "        \n",
    "        if i!=y_name:\n",
    "            mu = df[i].mean()\n",
    "            std = df[i].std()\n",
    "            for j in range(len(df[i])):\n",
    "                if df[i][j]<mu :\n",
    "                    df.loc[j,i] = max(df[i][j],mu-n*std)\n",
    "\n",
    "\n",
    "                if df[i][j]>mu:\n",
    "                    df.loc[j,i] = min(df[i][j],mu+n*std)\n",
    "        #label_counter+=1\n",
    "        #print(\"Dealing outlier (Lable):\",label_counter,\"\\\\\",total_labels)\n",
    "    return df\n",
    "\n",
    "# ===================================   for data processing   =================================== \n",
    "def z_score(df):\n",
    "    return (df-df.mean())/df.std()\n",
    "# 回傳標準化z-score後的表格(df)\n",
    "\n",
    "def log(df):\n",
    "    logdf = np.log(df+1)\n",
    "    logdf= logdf.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    return logdf.fillna(0)\n",
    "# 回傳log transform 結果，0保持在0\n",
    "\n",
    "def shuffle_data(df): \n",
    "    return df.sample(frac=1).reset_index(drop=True)\n",
    "# 隨機排序data\n",
    "\n",
    "def split_data(df,len_train_data):\n",
    "    len_all_data = len(df.index)\n",
    "    \n",
    "    s = pd.Series( [i for i in range(0,len_all_data-len_train_data)])\n",
    "    train = all_data.loc[:len_train_data]\n",
    "\n",
    "    test = all_data.loc[len_train_data:].set_index(s)\n",
    "    \n",
    "    return train,test\n",
    "# 將data 切成train與test,並且給定長度\n",
    "\n",
    "\n",
    "\n",
    "# plot the raw observations\n",
    "def cross_validation_plot(err_record):\n",
    "    \"\"\"\n",
    "    input : err_record : { size1 : [err1 , err2] , size2: [err1 , err2] ... }\n",
    "    output : print a plot  [Cross-validation error VS parameter size]\n",
    "    \"\"\"\n",
    "    parameter_size = list(err_record.keys())\n",
    "    for size in parameter_size:\n",
    "        errs = err_record[size]\n",
    "        plt.scatter([ size] * len(errs), errs)\n",
    "\n",
    "    # plot the trend line with error bars that correspond to standard deviation\n",
    "    err_mean = np.array([np.mean(v) for k,v in sorted(err_record.items())])\n",
    "    err_std = np.array([np.std(v) for k,v in sorted(err_record.items())])\n",
    "    plt.errorbar(parameter_size, err_mean ,  yerr=err_std)\n",
    "    plt.title('Cross-validation on parameter size')\n",
    "    plt.xlabel('parameter size')\n",
    "    plt.ylabel('Cross-validation error')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_y = train[\"SalePrice\"]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# data preprocessing  for  \n",
    "1.encoding cat data by median\n",
    "2.nan remove (by drop and replace)\n",
    "3.scaling (take z_score)\n",
    "4.outlier  \n",
    "\"\"\"\n",
    "# encoding\n",
    "(train_x,replace_dir) = catData_encoding(train,\"SalePrice\")\n",
    "# drop too many nan column \n",
    "train_x = drop_miss_rate(train_x , 0.2)\n",
    "# nan remove\n",
    "train_x = train_x.fillna(0)\n",
    "# outlier \n",
    "train_x = numData_outlier_limit(train_x,\"SalePrice\",3)\n",
    "print(train_x)\n",
    "# scaling \n",
    "train_x = z_score(train_x)\n",
    "\n",
    "# correlation\n",
    "train_x_label = numData_corr_filter(train_x ,\"SalePrice\",0.3)\n",
    "\n",
    "\n",
    "\n",
    "#train_x\n",
    "\n",
    "\n",
    "train_x = train_x[train_x_label]\n",
    "\n",
    "\n",
    "#temp[\"SalePrice\"] = train_y\n",
    "#numData_show_detail(temp,\"3SsnPorch\",\"SalePrice\")\n",
    "#print(train_x_corr[\"3SsnPorch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training part  -- with cross fold validation\n",
    "\n",
    "train_y_log = np.log(train_y.to_numpy())\n",
    "\n",
    "num_fold = 5\n",
    "print(train_x.shape)\n",
    "train_x_fold = np.array_split(train_x.to_numpy(),num_fold)\n",
    "train_y_fold = np.array_split(train_y_log,num_fold)\n",
    "\n",
    "\n",
    "test_h_size  = [i for i in range(2,10)]\n",
    "import sklearn\n",
    "\n",
    "#from sklearn.linear_model import Ridge\n",
    "#reg = Ridge(alpha=0.2)\n",
    "#from sklearn import linear_model\n",
    "#reg = linear_model.LinearRegression()\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#reg = LogisticRegression(max_iter=2000)\n",
    "#from sklearn import linear_model\n",
    "#reg = linear_model.ARDRegression()\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#reg = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "#from sklearn.ensemble import GradientBoostingRegressor\n",
    "#reg = GradientBoostingRegressor(n_estimators=2048, learning_rate=0.01,\\\n",
    "#max_depth=2, random_state=0, loss='ls')\n",
    "\n",
    "err_record = {} # 共有 #待測參數 個 list ,每個list含 #fold 筆error 資料\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "for h_size in test_h_size:\n",
    "                                                                  # target \n",
    "    reg = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=  h_size  ,max_iter=4000,alpha=1,verbose=False ,learning_rate=\"adaptive\")\n",
    "    \n",
    "#from sklearn import svm\n",
    "#reg=svm.SVR()\n",
    "#from sklearn import tree\n",
    "#reg = tree.DecisionTreeRegressor()\n",
    "#reg = sklearn.neural_network.MLPRegressor(tol=1e-7,learning_rate_init=0.06428,hidden_layer_sizes=(60,30,15,7,3,1),max_iter=15000,verbose=True)\n",
    "#print(train_x)\n",
    "#reg.fit(train_x, np.sqrt(train_y))\n",
    "    err_record[h_size]=[]\n",
    "    for n in range(num_fold):\n",
    "          \n",
    "     \n",
    "        train_x_split = np.concatenate(train_x_fold[:n]+train_x_fold[n+1:])\n",
    "        train_y_split = np.concatenate(train_y_fold[:n]+train_y_fold[n+1:])\n",
    "        reg.fit(train_x_split, train_y_split)\n",
    "        \n",
    "        val_hat = reg.predict(train_x_fold[n])\n",
    "        test_err = mean_squared_log_error(train_y_fold[n], val_hat)\n",
    "        err_record[h_size].append(test_err)\n",
    "        \n",
    "    print(err_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validation_plot(err_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose best hyperparameter\n",
    "best_size = test_h_size[err_mean.argmin()] # because less error is better\n",
    "print(best_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=512 ,max_iter=4000,alpha=0.6,verbose=False ,learning_rate=\"adaptive\")\n",
    "reg.fit(train_x , train_y_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "temp = {\"Id\":[],\"SalePrice\":[]}\n",
    "\n",
    "test_x = test\n",
    "# encoding cat data\n",
    "test_x = test_x.replace(replace_dir)\n",
    "# nan remove\n",
    "test_x = test_x.fillna(0)\n",
    "\n",
    "\n",
    "# get relative feature\n",
    "test_x = test_x[train_x_label]\n",
    "# oulier dealing\n",
    "test_x = numData_outlier_limit(test_x,\"\",3)\n",
    "test_x  = z_score(test_x)\n",
    "#print(test_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(num_data_test)\n",
    "\n",
    "#test_x  = log(num_data_test).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "\n",
    "test_y_hat = reg.predict(test_x)\n",
    "\n",
    "count = 0\n",
    "for i in test_y_hat:\n",
    "    \n",
    "    temp[\"Id\"].append(test[\"Id\"][count])\n",
    "    \n",
    "    #temp[\"SalePrice\"].append(i**2)\n",
    "    temp[\"SalePrice\"].append(np.exp(i))\n",
    "    count+=1\n",
    "\n",
    "    \n",
    "#print(temp)\n",
    "#i = np.random.randint(0,len(temp[\"Id\"]))\n",
    "i=0\n",
    "print(\"第\",i,\"筆\",temp[\"Id\"][i],temp[\"SalePrice\"][i])\n",
    "temp = pd.DataFrame(temp)\n",
    "\n",
    "#for i in range(len(temp[\"Id\"])):\n",
    "#    print(\"第一筆\",temp[\"Id\"][i],temp[\"SalePrice\"][i])\n",
    "\n",
    "\n",
    "\n",
    "#test_y \n",
    "#mean_squared_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_hat = reg.predict(train_x)\n",
    "\n",
    "for i in range(len(train_y_hat)):\n",
    "    #train_y_hat[i]=(train_y_hat[i])**2\n",
    "    train_y_hat[i]=np.exp(train_y_hat[i])\n",
    "for i in range(len(test_y_hat)):\n",
    "    #test_y_hat[i]=(test_y_hat[i])**2\n",
    "    test_y_hat[i]=np.exp(test_y_hat[i])    \n",
    "\n",
    "# validation variance error (overfitting) or bias error (underfiting)\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "train_err = mean_squared_log_error(train_y , train_y_hat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_y = pd.read_csv(\"solution.csv\")\n",
    "\n",
    "\n",
    "\n",
    "test_err = mean_squared_log_error(test_y[\"SalePrice\"] , test_y_hat)\n",
    "\n",
    "print(\"training error:\",train_err)\n",
    "print(\"testing error :\",test_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!del answer.csv\n",
    "temp.to_csv(\"answer.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving and loading part \n",
    "\n",
    "import joblib\n",
    "joblib.dump(reg,'svm_0.41.joblib')\n",
    "#reg2 =joblib.load(\"NNmodel_loss_0.18.pkl\")\n",
    "#reg2.predict(num_data_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble\n",
    "import sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#reg = sklearn.neural_network.MLPRegressor(max_iter=15000)\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn import tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "forest = RandomForestRegressor(max_depth=4, random_state=0)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=75, learning_rate=0.01,\\\n",
    "max_depth=1, random_state=0, loss='ls')\n",
    "\n",
    "#tree = tree.DecisionTreeRegressor()\n",
    "lr = linear_model.LinearRegression()\n",
    "lr_ridge  = Ridge(alpha=0.2)\n",
    "lr_bay  = linear_model.ARDRegression() \n",
    "#svm = svm.SVR()\n",
    "nn = sklearn.neural_network.MLPRegressor(hidden_layer_sizes=16,max_iter=4000,alpha=1,verbose=True ,learning_rate=\"adaptive\")\n",
    "ereg = VotingRegressor(estimators=[('nn',nn),('lr_ridge',lr_ridge),('lr_bay',lr_bay),(\"forest\",forest) ,('lr',lr),('gbr',gbr)])\n",
    "ereg = ereg.fit(train_x, np.log(train_y))\n",
    "\n",
    "test_y_hat = ereg.predict(test_x)\n",
    "\n",
    "temp = {\"Id\":[],\"SalePrice\":[]}\n",
    "count = 0\n",
    "\n",
    "for i in test_y_hat:\n",
    "    \n",
    "    temp[\"Id\"].append(test[\"Id\"][count])\n",
    "    \n",
    "    #temp[\"SalePrice\"].append((i)**2)\n",
    "    temp[\"SalePrice\"].append(np.exp(i))\n",
    "    count+=1\n",
    "\n",
    "    \n",
    "#print(temp)\n",
    "print(\"第一筆\",temp[\"Id\"][0],temp[\"SalePrice\"][0])\n",
    "temp = pd.DataFrame(temp)\n",
    "!del answer.csv\n",
    "temp.to_csv(\"answer.csv\",index=False)\n",
    "\n",
    "train_y_hat = ereg.predict(train_x)\n",
    "\n",
    "for i in range(len(train_y_hat)):\n",
    "    #train_y_hat[i]=(train_y_hat[i])**2\n",
    "    train_y_hat[i]=np.exp(train_y_hat[i])\n",
    "for i in range(len(test_y_hat)):\n",
    "    #test_y_hat[i]=(test_y_hat[i])**2\n",
    "    test_y_hat[i]=np.exp(test_y_hat[i])    \n",
    "\n",
    "# validation variance error (overfitting) or bias error (underfiting)\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "train_err = mean_squared_log_error(train_y , train_y_hat)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "test_y = pd.read_csv(\"solution.csv\")\n",
    "\n",
    "\n",
    "\n",
    "test_err = mean_squared_log_error(test_y[\"SalePrice\"] , test_y_hat)\n",
    "\n",
    "print(\"training error:\",train_err)\n",
    "print(\"testing error :\",test_err)\n",
    "\n",
    "for e in ereg.transform(test_x.iloc[:1,:]):\n",
    "    print(np.exp(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
